{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1569,"status":"ok","timestamp":1708699897843,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"0HShh4tgZ45A"},"outputs":[],"source":["# Importing necessary libraries\n","import os\n","import pandas as pd\n","import nltk\n","\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import random\n","import json\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":619,"status":"ok","timestamp":1708699898459,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"Xa9_fpUUcAuo","outputId":"c19a1dcc-50b6-4c0f-8a54-a8c17589d60e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Download NLTK resources\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2647,"status":"ok","timestamp":1708699901103,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"_t5niNxmbKAA"},"outputs":[],"source":["# Load data from CSV file into a DataFrame\n","data_link = \"/content/chatbot/Data/dialogues.csv\"\n","df = pd.read_csv(data_link)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1708699901103,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"rSK__A_kbJuY"},"outputs":[],"source":["data_path = \"/content/chatbot/Data/How_I met_your_mother_episodes_dialogues_parsed.json\""]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1166,"status":"ok","timestamp":1708699902264,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"3WXZBB9AaC1p"},"outputs":[],"source":["# Load data from JSON file into a DataFrame\n","with open(data_path, 'r', encoding='utf-8') as file:\n","    data = json.load(file)\n","\n","# Extracting text from the nested structure\n","text_list = []\n","for episode in data:\n","    for dialogue in episode[\"dialogues\"]:\n","        text_list.append(dialogue[\"text\"])"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":474,"status":"ok","timestamp":1708699936566,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"5KlU9wjCa28O"},"outputs":[],"source":["# Tokenization and TF-IDF Vectorization\n","def lemma_tokenizer(text):\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = word_tokenize(text)\n","    lemma_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in nltk_stopwords]\n","    return lemma_tokens"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":9698,"status":"ok","timestamp":1708700009587,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"paGio8zGbbYx"},"outputs":[],"source":["nltk_stopwords = set(stopwords.words('english'))  # Retrieve English stopwords from NLTK\n","nltk_stopwords = list(stopwords.words('english'))  # Convert the set of stopwords to a list\n","vectorizer = TfidfVectorizer(stop_words=nltk_stopwords, tokenizer=lemma_tokenizer, ngram_range=(1,2), max_features=5024)\n","matrix_tfidf = vectorizer.fit_transform(text_list)"]},{"cell_type":"code","execution_count":83,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708702785176,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"Zx2IvxVObgha"},"outputs":[],"source":["def get_relevant_phrase(text, mtx, text_list, relevantness=1.0, rel_random=0.05):\n","    # Transform the input text to TF-IDF vector\n","    query_vector = vectorizer.transform([text])\n","    # Calculate cosine similarities between the input text and all texts in the corpus\n","    cosine_similarities = cosine_similarity(query_vector, mtx).flatten()\n","    # Sort the indices of texts based on cosine similarities\n","    relevant_indices = np.argsort(cosine_similarities, axis=0)\n","    # Introduce randomness based on rel_random\n","    k_random = random.random() * rel_random\n","    relevantness = min(1, relevantness + k_random)\n","    # Calculate the index of the relevant text\n","    ind = relevant_indices[int((len(relevant_indices) - 1) * relevantness)]\n","    # Return the most relevant text\n","    return text_list[ind]"]},{"cell_type":"code","execution_count":97,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1708703349160,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"gqpSNv-Lp16i","outputId":"f4c95665-41c6-4248-b674-f1cdad370b52"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' And the diving suit?'"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["# Example usage\n","get_relevant_phrase(\"suit\", matrix_tfidf, text_list)"]},{"cell_type":"code","execution_count":84,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1708702786031,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"fALGOkdPh8kA"},"outputs":[],"source":["def get_cosine_similarity_label(question, answer, vectorizer):\n","    question_vector = vectorizer.transform([question])\n","    answer_vector = vectorizer.transform([answer])\n","\n","\n","    cosine_sim = cosine_similarity(question_vector, answer_vector)[0][0]\n","    label = int(cosine_sim * 9)\n","    return label\n"]},{"cell_type":"code","execution_count":87,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1708702795128,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"wHqEBni0jLdR"},"outputs":[],"source":["def get_cosine_similarity_label(question, answer, vectorizer):\n","    question_vector = vectorizer.transform([question])\n","    answer_vector = vectorizer.transform([answer])\n","\n","    # Add random noise to the cosine similarity\n","    cosine_sim = cosine_similarity(question_vector, answer_vector)[0][0]\n","    cosine_sim += np.random.uniform(-0.1, 0.1)\n","\n","    # Clip the cosine similarity to ensure it's within [0, 1]\n","    cosine_sim = np.clip(cosine_sim, 0, 1)\n","\n","    label = int(cosine_sim * 2)\n","    return label"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":482,"status":"ok","timestamp":1708703158166,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"AGKgNf2ejV60"},"outputs":[],"source":["def assign_label(row):\n","    q, a = row['Q'], row['A']\n","    relevant_phrase = get_relevant_phrase(q, matrix_tfidf, text_list)\n","    cosine_sim_label = get_cosine_similarity_label(q, relevant_phrase, vectorizer)\n","    return cosine_sim_label"]},{"cell_type":"code","execution_count":98,"metadata":{"executionInfo":{"elapsed":62850,"status":"ok","timestamp":1708703437769,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"},"user_tz":-180},"id":"sNrGxuR9aaeg"},"outputs":[],"source":["# Apply the assign_label function to the dataframe and create a new column \"label\"\n","df = df.dropna()\n","df['label'] = df[['Context', 'A']].apply(assign_label, axis=1)\n","\n","# Save the new dataframe with the new \"label\" column\n","df.to_csv('labeled_dataset.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMX90JoLAzhkIThq3qWouxK","gpuType":"T4","mount_file_id":"1t4xrJS174qfsWPxdUCHxxysK2KtLtziy","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
