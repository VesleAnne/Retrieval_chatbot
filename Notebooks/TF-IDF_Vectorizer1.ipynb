{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1t4xrJS174qfsWPxdUCHxxysK2KtLtziy","authorship_tag":"ABX9TyMX90JoLAzhkIThq3qWouxK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"0HShh4tgZ45A","executionInfo":{"status":"ok","timestamp":1708699897843,"user_tz":-180,"elapsed":1569,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}}},"outputs":[],"source":["# Importing necessary libraries\n","import os\n","import pandas as pd\n","import nltk\n","\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import random\n","import json\n"]},{"cell_type":"code","source":["# Download NLTK resources\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xa9_fpUUcAuo","executionInfo":{"status":"ok","timestamp":1708699898459,"user_tz":-180,"elapsed":619,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"outputId":"c19a1dcc-50b6-4c0f-8a54-a8c17589d60e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# Load data from CSV file into a DataFrame\n","data_link = \"/content/drive/MyDrive/МФТИ/nlp2/chatbot/Data/dialogues.csv\"\n","df = pd.read_csv(data_link)"],"metadata":{"id":"_t5niNxmbKAA","executionInfo":{"status":"ok","timestamp":1708699901103,"user_tz":-180,"elapsed":2647,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["data_path = \"/content/drive/MyDrive/МФТИ/nlp2/chatbot/Data/How_I met_your_mother_episodes_dialogues_parsed.json\""],"metadata":{"id":"rSK__A_kbJuY","executionInfo":{"status":"ok","timestamp":1708699901103,"user_tz":-180,"elapsed":6,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Load data from JSON file into a DataFrame\n","with open(data_path, 'r', encoding='utf-8') as file:\n","    data = json.load(file)\n","\n","# Extracting text from the nested structure\n","text_list = []\n","for episode in data:\n","    for dialogue in episode[\"dialogues\"]:\n","        text_list.append(dialogue[\"text\"])"],"metadata":{"id":"3WXZBB9AaC1p","executionInfo":{"status":"ok","timestamp":1708699902264,"user_tz":-180,"elapsed":1166,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Tokenization and TF-IDF Vectorization\n","def lemma_tokenizer(text):\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = word_tokenize(text)\n","    lemma_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in nltk_stopwords]\n","    return lemma_tokens"],"metadata":{"id":"5KlU9wjCa28O","executionInfo":{"status":"ok","timestamp":1708699936566,"user_tz":-180,"elapsed":474,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["nltk_stopwords = set(stopwords.words('english'))  # Retrieve English stopwords from NLTK\n","nltk_stopwords = list(stopwords.words('english'))  # Convert the set of stopwords to a list\n","vectorizer = TfidfVectorizer(stop_words=nltk_stopwords, tokenizer=lemma_tokenizer, ngram_range=(1,2), max_features=5024)\n","matrix_tfidf = vectorizer.fit_transform(text_list)"],"metadata":{"id":"paGio8zGbbYx","executionInfo":{"status":"ok","timestamp":1708700009587,"user_tz":-180,"elapsed":9698,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def get_relevant_phrase(text, mtx, text_list, relevantness=1.0, rel_random=0.05):\n","    # Transform the input text to TF-IDF vector\n","    query_vector = vectorizer.transform([text])\n","    # Calculate cosine similarities between the input text and all texts in the corpus\n","    cosine_similarities = cosine_similarity(query_vector, mtx).flatten()\n","    # Sort the indices of texts based on cosine similarities\n","    relevant_indices = np.argsort(cosine_similarities, axis=0)\n","    # Introduce randomness based on rel_random\n","    k_random = random.random() * rel_random\n","    relevantness = min(1, relevantness + k_random)\n","    # Calculate the index of the relevant text\n","    ind = relevant_indices[int((len(relevant_indices) - 1) * relevantness)]\n","    # Return the most relevant text\n","    return text_list[ind]"],"metadata":{"id":"Zx2IvxVObgha","executionInfo":{"status":"ok","timestamp":1708702785176,"user_tz":-180,"elapsed":4,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","get_relevant_phrase(\"suit\", matrix_tfidf, text_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"gqpSNv-Lp16i","executionInfo":{"status":"ok","timestamp":1708703349160,"user_tz":-180,"elapsed":23,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"outputId":"f4c95665-41c6-4248-b674-f1cdad370b52"},"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' And the diving suit?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["def get_cosine_similarity_label(question, answer, vectorizer):\n","    question_vector = vectorizer.transform([question])\n","    answer_vector = vectorizer.transform([answer])\n","\n","\n","    cosine_sim = cosine_similarity(question_vector, answer_vector)[0][0]\n","    label = int(cosine_sim * 9)\n","    return label\n"],"metadata":{"id":"fALGOkdPh8kA","executionInfo":{"status":"ok","timestamp":1708702786031,"user_tz":-180,"elapsed":3,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["def get_cosine_similarity_label(question, answer, vectorizer):\n","    question_vector = vectorizer.transform([question])\n","    answer_vector = vectorizer.transform([answer])\n","\n","    # Add random noise to the cosine similarity\n","    cosine_sim = cosine_similarity(question_vector, answer_vector)[0][0]\n","    cosine_sim += np.random.uniform(-0.1, 0.1)\n","\n","    # Clip the cosine similarity to ensure it's within [0, 1]\n","    cosine_sim = np.clip(cosine_sim, 0, 1)\n","\n","    label = int(cosine_sim * 2)\n","    return label"],"metadata":{"id":"wHqEBni0jLdR","executionInfo":{"status":"ok","timestamp":1708702795128,"user_tz":-180,"elapsed":3,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["def assign_label(row):\n","    q, a = row['Q'], row['A']\n","    relevant_phrase = get_relevant_phrase(q, matrix_tfidf, text_list)\n","    cosine_sim_label = get_cosine_similarity_label(q, relevant_phrase, vectorizer)\n","    return cosine_sim_label"],"metadata":{"id":"AGKgNf2ejV60","executionInfo":{"status":"ok","timestamp":1708703158166,"user_tz":-180,"elapsed":482,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["# Apply the assign_label function to the dataframe and create a new column \"label\"\n","df = df.dropna()\n","df['label'] = df[['Context', 'A']].apply(assign_label, axis=1)\n","\n","# Save the new dataframe with the new \"label\" column\n","df.to_csv('labeled_dataset.csv', index=False)"],"metadata":{"id":"sNrGxuR9aaeg","executionInfo":{"status":"ok","timestamp":1708703437769,"user_tz":-180,"elapsed":62850,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}}},"execution_count":98,"outputs":[]}]}